{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0e1afb-b2ac-43ff-8cb4-5d6f386cc3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü™ô Camada Silver - Criptoativos\n",
    "\n",
    "## üìÑ Descri√ß√£o\n",
    "Este notebook realiza o processamento incremental da camada Bronze para a camada Silver, consolidando dados hist√≥ricos e intradi√°rios em uma √∫nica tabela Delta do dom√≠nio de criptoativos.\n",
    "\n",
    "O pipeline utiliza **Structured Streaming com Delta Change Data Feed (CDF)** para garantir ingest√£o eficiente, incremental e confi√°vel. Durante a primeira execu√ß√£o, o notebook carrega o hist√≥rico completo dos ativos. Ap√≥s isso, realiza atualiza√ß√µes cont√≠nuas com base em novos dados recebidos diariamente.\n",
    "\n",
    "Al√©m disso, o notebook possui intelig√™ncia para detectar **novos ativos** adicionados na Bronze, realizando a carga hist√≥rica automaticamente, garantindo consist√™ncia e cobertura total do dom√≠nio.\n",
    "\n",
    "## üì• Entradas (Bronze)\n",
    "- `bronze_historico`: hist√≥rico de pre√ßos di√°rios via Yahoo Finance\n",
    "- `bronze_intradiario`: cota√ß√µes intradi√°rias via CoinGecko\n",
    "\n",
    "## üì§ Sa√≠da (Silver)\n",
    "- `silver.ativo_financeiro`: tabela Delta unificada contendo hist√≥rico e atualiza√ß√µes di√°rias de todos os criptoativos\n",
    "\n",
    "## ‚öôÔ∏è Tecnologias e Estrat√©gias\n",
    "- üîÑ Structured Streaming com `foreachBatch`\n",
    "- üß† Processamento incremental via `readChangeFeed`\n",
    "- ‚ö° Merge Upsert com Delta Lake\n",
    "- üîç Detec√ß√£o autom√°tica de novos ativos\n",
    "- ‚úÖ Checagem de exist√™ncia da tabela Silver e schema evolution autom√°tico\n",
    "- üóÇ Checkpoints para toler√¢ncia a falhas e reprocessamentos seguros\n",
    "\n",
    "---\n",
    "\n",
    "> üí° *Este pipeline est√° preparado para rodar continuamente em ambiente de produ√ß√£o, garantindo atualiza√ß√£o constante dos dados financeiros do dom√≠nio de criptoativos.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb36626-f828-4778-a6a0-d26fba572cf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import expr, col\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3dce94-9f73-49b2-ac3b-b987f88e5325",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Catalog"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create catalog if not exists lakehouse managed location 's3://databricks-9cwyoqzauqyermnrdpparb-cloud-storage-bucket/unity-catalog/1732645886098685'\")\n",
    "spark.sql(\"use catalog lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd63532-be51-4ade-9204-b1e5ea7ff905",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema & Volumes"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create schema if not exists silver\")\n",
    "spark.sql(\"create volume if not exists silver.checkpoint_cripto\")\n",
    "spark.sql(\"create volume if not exists silver.schema_cripto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c236e675-6755-4d3d-99c4-7b4ba5384d46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Path"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/Volumes/lakehouse/silver/checkpoint_cripto\" \n",
    "schema_path = \"/Volumes/lakehouse/silver/schema_cripto\"\n",
    "table_name = \"silver.cripto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608b7207-e143-4cf9-9b50-942f84bb91f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Leitura com Structured Streaming e Change Data Feed (CDF) dos dados da camada Bronze\n",
    "# =====================================================================\n",
    "\n",
    "# L√™ as mudan√ßas da tabela bronze_historico desde a primeira vers√£o (full load na primeira execu√ß√£o)\n",
    "df_historico = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .table(\"bronze.historico_cripto\")\n",
    ")\n",
    "# L√™ as mudan√ßas da tabela bronze_intradiario\n",
    "df_intradiario = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .table(\"bronze.diario_cripto\")\n",
    ")\n",
    "\n",
    "# Ajuste do nome das colunas para facilitar a manipula√ß√£o\n",
    "df_historico = df_historico.withColumn(\"asset_name\", col(\"ticker\")).drop(col(\"ticker\")).withColumn(\"time_interval\",col(\"timestamp\"))\n",
    "\n",
    "# Acrescenta uma coluna para diferenciar a origem do dado (opcional, mas √∫til para debug/an√°lise)\n",
    "df_historico = df_historico.withColumn(\"source_type\", expr(\"'historico'\"))\n",
    "df_intradiario = df_intradiario.withColumn(\"source_type\", expr(\"'intradiario'\"))\n",
    "\n",
    "# Une os dois streams, garantindo que os schemas sejam compat√≠veis\n",
    "df_union = df_historico.unionByName(df_intradiario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37643c51-2e42-4930-b4c6-5babf00a7388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_to_silver(microBatchDF: DataFrame, batchId: int):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para inserir (append) os registros do microbatch na tabela Silver.\n",
    "    Como as tabelas Bronze s√£o insertOnly e o Structured Streaming capta apenas os registros novos,\n",
    "    n√£o √© necess√°rio fazer merge (upsert).\n",
    "    \"\"\"\n",
    "    microBatchDF.write.format(\"delta\").mode(\"append\").saveAsTable(\"silver.cripto\")\n",
    "    print(f\"Batch {batchId} inserido com sucesso na camada Silver.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485e1396-c780-463e-beda-b015ad51fa6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================\n",
    "# Configura o streaming para processar incrementalmente na Silver\n",
    "# =====================================================================\n",
    "\n",
    "silver_stream = (\n",
    "    df_union.writeStream\n",
    "            .option(\"checkpointLocation\", checkpoint_path)\n",
    "            .foreachBatch(insert_to_silver)\n",
    "            .outputMode(\"append\")  # Pode ser \"update\" ou \"append\" dependendo da l√≥gica de merge\n",
    "            .trigger(availableNow=True)\n",
    "            .start()\n",
    ")\n",
    "\n",
    "silver_stream.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8b0e3f-9f81-44ec-9a59-77f459584691",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setando Liquid Cluster Autom√°tico"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"alter table silver.cripto cluster by AUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa154562-28c9-48d2-b79b-88eeec9725e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aplicando otmiza√ß√£o na tabela"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"optimize silver.cripto\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "processamentoDominioCripto",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
